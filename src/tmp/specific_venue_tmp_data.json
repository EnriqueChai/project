{
  "results": {
    "venue": {
      "auto_id": 3124,
      "name": "CVPR",
      "type": "venue"
    },
    "papers": [
      {
        "auto_id": 260605,
        "id": 2161969291,
        "title": "histograms of oriented gradients for human detection",
        "type": "paper",
        "time": 2005,
        "citation": 3730,
        "abstract": "We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.",
        "author": "navneet dalal,bill triggs",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 265141,
        "id": 2108598243,
        "title": "imagenet a large scale hierarchical image database",
        "type": "paper",
        "time": 2009,
        "citation": 3259,
        "abstract": "The explosion of image data on the Internet has the potential to foster more sophisticated and robust models and algorithms to index, retrieve, organize and interact with images and multimedia data. But exactly how such data can be harnessed and organized remains a critical problem. We introduce here a new database called “ImageNet”, a large-scale ontology of images built upon the backbone of the WordNet structure. ImageNet aims to populate the majority of the 80,000 synsets of WordNet with an average of 500-1000 clean and full resolution images. This will result in tens of millions of annotated images organized by the semantic hierarchy of WordNet. This paper offers a detailed analysis of ImageNet in its current state: 12 subtrees with 5247 synsets and 3.2 million images in total. We show that ImageNet is much larger in scale and diversity and much more accurate than the current image datasets. Constructing such a large-scale database is a challenging task. We describe the data collection scheme with Amazon Mechanical Turk. Lastly, we illustrate the usefulness of ImageNet through three simple applications in object recognition, image classification and automatic object clustering. We hope that the scale, accuracy, diversity and hierarchical structure of ImageNet can offer unparalleled opportunities to researchers in the computer vision community and beyond.",
        "author": "jun deng,kehui li,lijia li,richard socher,wenjie dong,li feifei",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 263433,
        "id": 2194775991,
        "title": "deep residual learning for image recognition",
        "type": "paper",
        "time": 2016,
        "citation": 2630,
        "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers—8× deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.",
        "author": "kaiming he,xiangyu zhang,shaoqing ren,jian sun",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 264790,
        "id": 2102605133,
        "title": "rich feature hierarchies for accurate object detection and semantic segmentation",
        "type": "paper",
        "time": 2014,
        "citation": 2409,
        "abstract": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn.",
        "author": "ross b girshick,jeff donahue,trevor darrell,jitendra malik",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 262908,
        "id": 2097117768,
        "title": "going deeper with convolutions",
        "type": "paper",
        "time": 2015,
        "citation": 1897,
        "abstract": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.",
        "author": "christian szegedy,wei liu,scott e reed,dumitru erhan,yangqing jia,dragomir anguelov,pierre sermanet,vincent vanhoucke",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 262681,
        "id": 2162915993,
        "title": "beyond bags of features spatial pyramid matching for recognizing natural scene categories",
        "type": "paper",
        "time": 2006,
        "citation": 1545,
        "abstract": "This paper presents a method for recognizing scene categories based on approximate global geometric correspondence. This technique works by partitioning the image into increasingly fine sub-regions and computing histograms of local features found inside each sub-region. The resulting \"spatial pyramid\" is a simple and computationally efficient extension of an orderless bag-of-features image representation, and it shows significantly improved performance on challenging scene categorization tasks. Specifically, our proposed method exceeds the state of the art on the Caltech-101 database and achieves high accuracy on a large database of fifteen natural scene categories. The spatial pyramid framework also offers insights into the success of several recently proposed image descriptions, including Torralbas \"gist\" and Lowes SIFT descriptors.",
        "author": "svetlana lazebnik,cordelia schmid,jean ponce",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 261686,
        "id": 2145287260,
        "title": "deepface closing the gap to human level performance in face verification",
        "type": "paper",
        "time": 2014,
        "citation": 873,
        "abstract": "In modern face recognition, the conventional pipeline consists of four stages: detect =\u003E align =\u003E represent =\u003E classify. We revisit both the alignment step and the representation step by employing explicit 3D face modeling in order to apply a piecewise affine transformation, and derive a face representation from a nine-layer deep neural network. This deep network involves more than 120 million parameters using several locally connected layers without weight sharing, rather than the standard convolutional layers. Thus we trained it on the largest facial dataset to-date, an identity labeled dataset of four million facial images belonging to more than 4, 000 identities. The learned representations coupling the accurate model-based alignment with the large facial database generalize remarkably well to faces in unconstrained environments, even with a simple classifier. Our method reaches an accuracy of 97.35% on the Labeled Faces in the Wild (LFW) dataset, reducing the error of the current state of the art by more than 27%, closely approaching human-level performance.",
        "author": "yaniv taigman,marcaurelio ranzato,ming wei yang,lior wolf",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 261651,
        "id": 2128017662,
        "title": "scalable recognition with a vocabulary tree",
        "type": "paper",
        "time": 2006,
        "citation": 781,
        "abstract": "A recognition scheme that scales efficiently to a large number of objects is presented. The efficiency and quality is exhibited in a live demonstration that recognizes CD-covers from a database of 40000 images of popular music CDs. The scheme builds upon popular techniques of indexing descriptors extracted from local regions, and is robust to background clutter and occlusion. The local region descriptors are hierarchically quantized in a vocabulary tree. The vocabulary tree allows a larger and more discriminatory vocabulary to be used efficiently, which we show experimentally leads to a dramatic improvement in retrieval quality. The most significant property of the scheme is that the tree directly defines the quantization. The quantization and the indexing are therefore fully integrated, essentially being one and the same. The recognition quality is evaluated through retrieval on a database with ground truth, showing the power of the vocabulary tree approach, going as high as 1 million images.",
        "author": "david nister,henrik stewenius",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 265132,
        "id": 2150066425,
        "title": "are we ready for autonomous driving the kitti vision benchmark suite",
        "type": "paper",
        "time": 2012,
        "citation": 754,
        "abstract": "Today, visual recognition systems are still rarely employed in robotics applications. Perhaps one of the main reasons for this is the lack of demanding benchmarks that mimic such scenarios. In this paper, we take advantage of our autonomous driving platform to develop novel challenging benchmarks for the tasks of stereo, optical flow, visual odometry/SLAM and 3D object detection. Our recording platform is equipped with four high resolution video cameras, a Velodyne laser scanner and a state-of-the-art localization system. Our benchmarks comprise 389 stereo and optical flow image pairs, stereo visual odometry sequences of 39.2 km length, and more than 200k 3D object annotations captured in cluttered scenarios (up to 15 cars and 30 pedestrians are visible per image). Results from state-of-the-art algorithms reveal that methods ranking high on established datasets such as Middlebury perform below average when being moved outside the laboratory to the real world. Our goal is to reduce this bias by providing challenging benchmarks with novel difficulties to the computer vision community. Our benchmarks are available online at: www.cvlibs.net/datasets/kitti",
        "author": "andreas geiger,philip lenz,raquel urtasun",
        "venue_name": "CVPR",
        "venue_type": "venue"
      },
      {
        "auto_id": 263662,
        "id": 2016053056,
        "title": "large scale video classification with convolutional neural networks",
        "type": "paper",
        "time": 2014,
        "citation": 711,
        "abstract": "Convolutional Neural Networks (CNNs) have been established as a powerful class of models for image recognition problems. Encouraged by these results, we provide an extensive empirical evaluation of CNNs on large-scale video classification using a new dataset of 1 million YouTube videos belonging to 487 classes. We study multiple approaches for extending the connectivity of a CNN in time domain to take advantage of local spatio-temporal information and suggest a multiresolution, foveated architecture as a promising way of speeding up the training. Our best spatio-temporal networks display significant performance improvements compared to strong feature-based baselines (55.3% to 63.9%), but only a surprisingly modest improvement compared to single-frame models (59.3% to 60.9%). We further study the generalization performance of our best model by retraining the top layers on the UCF-101 Action Recognition dataset and observe significant performance improvements compared to the UCF-101 baseline model (63.3% up from 43.9%).",
        "author": "andrej karpathy,george toderici,rahul sukthankar,sanketh shetty,thomas leung,li feifei",
        "venue_name": "CVPR",
        "venue_type": "venue"
      }
    ],
    "authors": [
      {
        "auto_id": 2918,
        "id": 1984838606,
        "name": "li feifei",
        "citation": 13377,
        "field": "Categorization"
      },
      {
        "auto_id": 32899,
        "id": 2200192130,
        "name": "jian sun",
        "citation": 12419,
        "field": "Scalability"
      },
      {
        "auto_id": 7131,
        "id": 2473549963,
        "name": "ross b girshick",
        "citation": 12070,
        "field": "Speedup"
      },
      {
        "auto_id": 323,
        "id": 2174985400,
        "name": "trevor darrell",
        "citation": 10471,
        "field": "Real-time operating system"
      },
      {
        "auto_id": 17462,
        "id": 1964982643,
        "name": "richard socher",
        "citation": 8705,
        "field": "Paraphrase"
      },
      {
        "auto_id": 78180,
        "id": 2164292938,
        "name": "kaiming he",
        "citation": 8377,
        "field": "Pyramid"
      },
      {
        "auto_id": 9540,
        "id": 2136556746,
        "name": "jitendra malik",
        "citation": 7934,
        "field": "RGB color model"
      },
      {
        "auto_id": 96794,
        "id": 2133088636,
        "name": "jeff donahue",
        "citation": 6393,
        "field": "Classifier"
      },
      {
        "auto_id": 115198,
        "id": 2119543935,
        "name": "shaoqing ren",
        "citation": 6032,
        "field": "Mobile phone"
      },
      {
        "auto_id": 73151,
        "id": 2111851554,
        "name": "cordelia schmid",
        "citation": 5421,
        "field": "Repeatability"
      }
    ],
    "recommended_reviewers": {
      "Artificial neural network": [
        {
          "auto_id": 26831,
          "id": 331124168,
          "name": "christian szegedy",
          "citation": 4822,
          "field": "Artificial neural network"
        },
        {
          "auto_id": 26835,
          "id": 2022840042,
          "name": "vincent vanhoucke",
          "citation": 3620,
          "field": "Artificial neural network"
        },
        {
          "auto_id": 26836,
          "id": 37905947,
          "name": "pierre sermanet",
          "citation": 2494,
          "field": "Artificial neural network"
        }
      ],
      "Baseline": [
        {
          "auto_id": 26829,
          "id": 2617788377,
          "name": "wei liu",
          "citation": 2134,
          "field": "Baseline"
        }
      ],
      "Biometrics": [
        {
          "auto_id": 5924,
          "id": 2291154966,
          "name": "raquel urtasun",
          "citation": 2550,
          "field": "Biometrics"
        }
      ],
      "Bootstrapping": [
        {
          "auto_id": 26830,
          "id": 2430826622,
          "name": "scott e reed",
          "citation": 2815,
          "field": "Bootstrapping"
        }
      ],
      "Computer vision": [
        {
          "auto_id": 467795,
          "id": 2170199860,
          "name": "navneet dalal",
          "citation": 3980,
          "field": "Computer vision"
        }
      ],
      "Formalism": [
        {
          "auto_id": 5017,
          "id": 2138495596,
          "name": "jean ponce",
          "citation": 3554,
          "field": "Formalism"
        }
      ],
      "Geodetic datum": [
        {
          "auto_id": 58669,
          "id": 2057410742,
          "name": "bill triggs",
          "citation": 5069,
          "field": "Geodetic datum"
        }
      ],
      "Image retrieval": [
        {
          "auto_id": 25535,
          "id": 2115844506,
          "name": "lijia li",
          "citation": 4444,
          "field": "Image retrieval"
        },
        {
          "auto_id": 125800,
          "id": 46106380,
          "name": "andrej karpathy",
          "citation": 4106,
          "field": "Image retrieval"
        },
        {
          "auto_id": 154845,
          "id": 2967563870,
          "name": "jun deng",
          "citation": 3369,
          "field": "Image retrieval"
        },
        {
          "auto_id": 154844,
          "id": 2968554593,
          "name": "kehui li",
          "citation": 3259,
          "field": "Image retrieval"
        },
        {
          "auto_id": 154846,
          "id": 2969250883,
          "name": "wenjie dong",
          "citation": 3259,
          "field": "Image retrieval"
        }
      ],
      "Liquid-crystal display": [
        {
          "auto_id": 1463,
          "id": 1436821675,
          "name": "rahul sukthankar",
          "citation": 2299,
          "field": "Liquid-crystal display"
        }
      ],
      "Markov chain": [
        {
          "auto_id": 26834,
          "id": 2036321405,
          "name": "dragomir anguelov",
          "citation": 2214,
          "field": "Markov chain"
        }
      ],
      "Mobile phone": [
        {
          "auto_id": 115198,
          "id": 2119543935,
          "name": "shaoqing ren",
          "citation": 6032,
          "field": "Mobile phone"
        }
      ],
      "Monte Carlo method": [
        {
          "auto_id": 26833,
          "id": 2102002796,
          "name": "yangqing jia",
          "citation": 5080,
          "field": "Monte Carlo method"
        }
      ],
      "Motion estimation": [
        {
          "auto_id": 62521,
          "id": 2112161334,
          "name": "david nister",
          "citation": 1359,
          "field": "Motion estimation"
        }
      ],
      "Numeral system": [
        {
          "auto_id": 37236,
          "id": 1976950599,
          "name": "marcaurelio ranzato",
          "citation": 3237,
          "field": "Numeral system"
        }
      ],
      "Pyramid": [
        {
          "auto_id": 160948,
          "id": 2499063207,
          "name": "xiangyu zhang",
          "citation": 4886,
          "field": "Pyramid"
        }
      ],
      "Repeatability": [
        {
          "auto_id": 73151,
          "id": 2111851554,
          "name": "cordelia schmid",
          "citation": 5421,
          "field": "Repeatability"
        }
      ],
      "Semi-supervised learning": [
        {
          "auto_id": 26832,
          "id": 2308824398,
          "name": "dumitru erhan",
          "citation": 4271,
          "field": "Semi-supervised learning"
        }
      ],
      "Similarity measure": [
        {
          "auto_id": 20188,
          "id": 2059111593,
          "name": "lior wolf",
          "citation": 2352,
          "field": "Similarity measure"
        }
      ],
      "Spin-½": [
        {
          "auto_id": 60035,
          "id": 7233622,
          "name": "svetlana lazebnik",
          "citation": 4105,
          "field": "Spin-½"
        }
      ]
    }
  }
}